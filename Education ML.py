# -*- coding: utf-8 -*-
"""Education ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UiRH1hpxHk5FkiagqH3ZOpCETHO_yrey

# **Preprocessing**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from scipy import stats

from google.colab import files
import io
import pandas as pd


uploaded = files.upload()
for file_name in uploaded.keys():
    data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=',')

raw_data = pd.read_csv('/content/Raw Data.csv', delimiter=',')

raw_data.head()

print(raw_data.info())

"""Scaling, Encoding and Removing null values"""

# Identify features and target
categorical_features = ['gender', 'major', 'work_industry']  # Exclude 'race' for now
numerical_features = ['gpa', 'gmat', 'work_exp']  # Numerical features
target = 'race'

# Separate rows with and without missing 'race' values
df_not_null = raw_data[raw_data[target].notnull()]
df_null = raw_data[raw_data[target].isnull()]

# Prepare the features and target for the non-null dataset
X_not_null = df_not_null[categorical_features + numerical_features]
y_not_null = df_not_null[target]

# Create a Column Transformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),  # Standard scaling for numerical features
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # One-hot encoding for categorical features
    ]
)

# Preprocess the non-null data
X_not_null_processed = preprocessor.fit_transform(X_not_null)

# Train a Decision Tree Classifier
model = DecisionTreeClassifier(random_state=42)
model.fit(X_not_null_processed, y_not_null)

# Prepare the features for the null dataset
X_null = df_null[categorical_features + numerical_features]
X_null_processed = preprocessor.transform(X_null)

# Predict missing 'race' values
predicted_race = model.predict(X_null_processed)

# Fill missing values in 'race'
raw_data.loc[raw_data[target].isnull(), target] = predicted_race

# Verify that there are no more missing values in 'race'
print(raw_data['race'].isnull().sum())

print(raw_data.info())

raw_data.head()

"""Code to Standardize Features Excluding Admissions"""

'''
# Remove the 'admission' feature
data = raw_data.copy()
data = data.drop(columns=['admission'])

# Identify features
categorical_features = ['gender', 'major', 'race', 'work_industry', 'international']  # Categorical features
numerical_features = ['gpa', 'gmat', 'work_exp']  # Numerical features

# Step 1: Label Encode Categorical Features
label_encoders = {}
for feature in categorical_features:
    le = LabelEncoder()
    data[feature] = le.fit_transform(data[feature].astype(str))  # Convert to string to handle NaN
    label_encoders[feature] = le  # Store the encoder

# Step 2: Standard Scale Numerical Features
num_scaler = StandardScaler()
X_num_scaled = num_scaler.fit_transform(data[numerical_features])

# Step 3: Standard Scale Categorical Features (after label encoding)
cat_scaler = StandardScaler()
X_cat_scaled = cat_scaler.fit_transform(data[categorical_features])

# Step 4: Concatenate the Scaled Numerical and Categorical Features
X_processed = np.concatenate((X_num_scaled, X_cat_scaled), axis=1)

# Combine the feature names
all_feature_names = numerical_features + categorical_features

# Create a DataFrame with the processed data
data = pd.DataFrame(X_processed, columns=all_feature_names)

# Display the first few rows of the processed DataFrame
print(data.head())
'''

"""Code to Standardize Features Excluding  Encoded Admissions"""

# Assume raw_data is the original DataFrame
data = raw_data.copy()

# Identify features
categorical_features = ['gender', 'major', 'race', 'work_industry', 'international']  # Categorical features (without 'admission')
numerical_features = ['gpa', 'gmat', 'work_exp']  # Numerical features

# Step 1: Encode the 'admission' feature as 0 for 'waitlist' and 1 for 'admit'
data['admission'] = data['admission'].map({'Waitlist': 0, 'Admit': 1})

# Step 2: Label Encode other Categorical Features
label_encoders = {}
for feature in categorical_features:
    le = LabelEncoder()
    data[feature] = le.fit_transform(data[feature].astype(str))  # Convert to string to handle NaN if any
    label_encoders[feature] = le  # Store the encoder for future use

# Step 3: Standard Scale Numerical Features
num_scaler = StandardScaler()
X_num_scaled = num_scaler.fit_transform(data[numerical_features])

# Step 4: Standard Scale Categorical Features (excluding 'admission')
cat_scaler = StandardScaler()
X_cat_scaled = cat_scaler.fit_transform(data[categorical_features])

# Step 5: Concatenate the Scaled Numerical and Categorical Features, but keep 'admission' as is
X_processed = np.concatenate((X_num_scaled, X_cat_scaled, data[['admission']]), axis=1)

# Combine the feature names (adding 'admission' at the end)
all_feature_names = numerical_features + categorical_features + ['admission']

# Create a DataFrame with the processed data
processed_data = pd.DataFrame(X_processed, columns=all_feature_names)

# Display the first few rows of the processed DataFrame
print(processed_data.head())

from google.colab import files

processed_data.to_csv('data.csv', index=False)
files.download('data.csv')

"""# **EDA**"""

# Compute the correlation matrix
correlation_matrix = data.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

# Add title
plt.title('Correlation Matrix of Features', size=15)

# Show the plot
plt.show()

"""# **Clustering**"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy import stats
import numpy as np

# Set the style of seaborn plots
sns.set(style="whitegrid")

# Plot box plots for each feature in the dataset
# We will exclude the 'Cluster' column if already added, and any non-numeric columns
features = data.select_dtypes(include=[np.number]).columns.tolist()  # Select numeric columns

plt.figure(figsize=(16, 12))  # Adjust size for better readability
for i, feature in enumerate(features, 1):
    plt.subplot(3, 3, i)  # Adjust subplot grid as per the number of features
    sns.boxplot(data=data, x=feature)
    plt.title(f'Box plot of {feature}')

plt.tight_layout()
plt.show()

"""Evaluate the clusters using elbow method"""

wcss = []
for i in range(1, 10):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 10), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Remove outliers based on Z-Score threshold (e.g., |z| > 3 is considered an outlier)
z_scores = np.abs(stats.zscore(data))  # Get absolute z-scores
filtered_entries = (z_scores < 3).all(axis=1)  # Filter rows where all z-scores are within 3
data_filtered = data[filtered_entries]  # Create a filtered DataFrame

# Apply K-Means Clustering with k-means++ initialization
kmeans = KMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=300, random_state=42)
kmeans.fit(data_filtered)

# Map the cluster labels to 'Waitlist' and 'Admit'
data_filtered['Admission'] = np.where(kmeans.labels_ == 0, 'Waitlist', 'Admit')

# Visualize Clusters using PCA (reducing to 2D for visualization purposes)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(data_filtered.drop(columns=['Admission']))  # Exclude 'Admission' for PCA

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', marker='o', edgecolor='k', s=50)
plt.title('Clustering Results (K-Means)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
plt.show()

unique, counts = np.unique(kmeans.labels_, return_counts=True)
cluster_sizes = dict(zip(unique, counts))
print("Cluster sizes:", cluster_sizes)

data_filtered['Cluster'] = kmeans.labels_

# Calculate the means of each feature for each admission status and cluster
cluster_means = data_filtered.groupby(['Cluster', 'Admission']).mean().reset_index()

# Display the means
print(cluster_means)

# Calculate the count of each admission status
admission_counts = data_filtered['Admission'].value_counts()

# Calculate the total number of entries
total_count = admission_counts.sum()

# Calculate the percentages
admission_percentages = (admission_counts / total_count) * 100

# Display the results
print("Admission Status Counts:")
print(admission_counts)
print("\nAdmission Status Percentages:")
print(admission_percentages)

from google.colab import files

data_filtered.to_csv('data.csv', index=False)
files.download('data.csv')

"""# **Supervised Learning Using Clustered 'Admission' Results**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report

from google.colab import files
import io
import pandas as pd


uploaded = files.upload()
for file_name in uploaded.keys():
    data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=',')

data.head()

data['Admission'] = data['Admission'].map({'Waitlist': 0, 'Admit': 1})

# Define features (X) and target (y)
X = data.drop(columns=['Admission'])  # Features (everything except the target column)
y = data['Admission']  # Target column

# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Build the deep learning model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer
    layers.Dropout(0.5),  # Dropout to avoid overfitting
    layers.Dense(32, activation='relu'),  # Hidden layer
    layers.Dropout(0.5),  # Another Dropout layer
    layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=5,
                    batch_size=32,
                    callbacks=[early_stopping])

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Define features (X) and target (y)
X = data.drop(columns=['Admission'])  # Features (everything except the target column)
y = data['Admission']  # Target column

# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Build the deep learning model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer
    layers.Dropout(0.5),  # Dropout to avoid overfitting
    layers.Dense(32, activation='relu'),  # Hidden layer
    layers.Dropout(0.5),  # Another Dropout layer
    layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=5,
                    batch_size=32,
                    callbacks=[early_stopping])

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Define features (X) and target (y)
X = data.drop(columns=['Admission'])  # Features (everything except the target column)
y = data['Admission']  # Target column

# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Build the deep learning model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer
    layers.Dropout(0.5),  # Dropout to avoid overfitting
    layers.Dense(32, activation='relu'),  # Hidden layer
    layers.Dropout(0.5),  # Another Dropout layer
    layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=2,
                    batch_size=32,
                    callbacks=[early_stopping])

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")

# After evaluating the model on the test set, generate predictions
y_pred = (model.predict(X_test) > 0.5).astype("int32")  # Convert probabilities to binary predictions

# Generate a classification report (including precision, recall, and F1-score)
report = classification_report(y_test, y_pred, target_names=['Waitlist', 'Admit'])
print(report)